"""
Train Logistic Regression with Gradient Descent

Implement a gradient descent-based training algorithm for logistic regression. Your task is to compute model parameters using Binary Cross Entropy loss and return the optimized coefficients along with collected loss values over iterations(round to the 4th decimal).

Example:
Input:
train_logreg(np.array([[1.0, 0.5], [-0.5, -1.5], [2.0, 1.5], [-2.0, -1.0]]), np.array([1, 0, 1, 0]), 0.01, 20)
Output:
([0.0037, 0.0246, 0.0202], [2.7726, 2.7373, 2.7024, 2.6678, 2.6335, 2.5995, 2.5659, 2.5327, 2.4997, 2.4671, 2.4348, 2.4029, 2.3712, 2.3399, 2.3089, 2.2783, 2.2480, 2.2180, 2.1882, 2.1588])
Reasoning:
The function iteratively updates the logistic regression parameters using gradient descent and collects loss values over iterations.

Learn About topic
Overview
Logistic regression is a model used for a binary classification poblem.

Prerequisites for a regular logistic regression
Logistic regression is based on the concept of "logits of odds". Odds is measure of how frequent we encounter success. It also allows us to shift our probabilities domain of 
[
0
,
1
]
[0,1] to 
[
0
,
∞
]
[0,∞] Consider a probability of scoring a goal 
p
=
0.8
p=0.8, then our 
o
d
d
s
=
0.8
0.2
=
4
odds= 
0.2
0.8
​
 =4. This means that every 
4
4 matches we could be expecting a goal followed by a miss. So the higher the odds, the more consistent is our streak of goals. Logit is an inverse of the standard logistic function, i.e. sigmoid: 
l
o
g
i
t
(
p
)
=
σ
−
1
(
p
)
=
l
n
p
1
−
p
logit(p)=σ 
−1
 (p)=ln 
1−p
p
​
 . In our case 
p
p is a probability, therefore we call 
p
1
−
p
1−p
p
​
  the "odds". The logit allows us to further expand our domain from 
[
0
,
∞
]
[0,∞] to 
[
−
∞
,
∞
]
[−∞,∞].

With this domain expansion we can treat our problem as a linear regression and try to approximate our logit function: 
X
β
=
l
o
g
i
t
(
p
)
Xβ=logit(p). However what we really want for this approximation is to yield predictions for probabilities:

X
β
=
l
n
p
1
−
p
e
−
X
β
=
1
−
p
p
e
−
X
β
+
1
=
1
p
p
=
1
e
−
X
β
+
1
Xβ=ln 
1−p
p
​
 
e 
−Xβ
 = 
p
1−p
​
 
e 
−Xβ
 +1= 
p
1
​
 
p= 
e 
−Xβ
 +1
1
​
 
What we practically just did is taking an inverse of a logit function w.r.t. our approximation and go back to sigmoid. This is also the backbone of the regular logistic regression, which is commonly defined as:

π
=
e
α
+
X
β
1
+
e
α
+
X
β
=
1
1
+
e
−
(
α
+
X
β
)
.
π= 
1+e 
α+Xβ
 
e 
α+Xβ
 
​
 = 
1+e 
−(α+Xβ)
 
1
​
 .
Loss in logistic regression
The loss function used for solving the logistic regression for 
β
β is derived from MLE (Maximum Likelihood Estimation). This method allows us to search for 
β
β that maximize our likelihood function 
L
(
β
)
L(β). This function tells us how likely it is that 
X
X has come from the distribution generated by 
β
β: 
L
(
β
)
=
L
(
β
∣
X
)
=
P
(
X
∣
β
)
=
∏
{
x
∈
X
}
f
X
u
n
i
v
a
r
(
x
;
β
)
L(β)=L(β∣X)=P(X∣β)=∏ 
{x∈X}
​
 f 
X
univar
​
 (x;β), where 
f
f is a PMF and 
u
n
i
v
a
r
univar means univariate, i.e. applied to a single variable.

In the case of a regular logistic regression we expect our output to belong to a single Bernoulli-distributed random variable (hence the univariance), since our true label is either 
y
i
=
0
y 
i
​
 =0 or 
y
i
=
1
y 
i
​
 =1. The Bernoulli's PMF is defined as 
P
(
Y
=
y
)
=
p
y
(
1
−
p
)
(
1
−
y
)
P(Y=y)=p 
y
 (1−p) 
(1−y)
 , where 
y
∈
{
0
,
1
}
y∈{0,1}. Also let's denote 
{
x
∈
X
}
{x∈X} simply as 
X
X and refer to a single pair of vectors from the training set as 
(
x
i
,
y
i
)
(x 
i
​
 ,y 
i
​
 ). Thus, our likelihood function would look like this:

∏
X
p
(
x
i
)
y
i
×
[
1
−
p
(
x
i
)
]
1
−
y
i
X
∏
​
 p(x 
i
​
 ) 
y 
i
​
 
 ×[1−p(x 
i
​
 )] 
1−y 
i
​
 
 
Then we convert our function from likelihood to log-likelihood by taking 
l
n
ln (or 
l
o
g
log) of it:

∑
X
y
i
log
⁡
[
p
(
x
i
)
]
+
(
1
−
y
i
)
log
⁡
[
1
−
p
(
x
i
)
]
X
∑
​
 y 
i
​
 log[p(x 
i
​
 )]+(1−y 
i
​
 )log[1−p(x 
i
​
 )]
And then we replace 
p
(
x
i
)
p(x 
i
​
 ) with the sigmoid from previously defined equality to get a final version of our loss function:

∑
X
y
i
log
⁡
(
1
1
+
e
−
x
i
β
)
+
(
1
−
y
i
)
log
⁡
(
1
−
1
1
+
e
−
x
i
β
)
X
∑
​
 y 
i
​
 log( 
1+e 
−x 
i
​
 β
 
1
​
 )+(1−y 
i
​
 )log(1− 
1+e 
−x 
i
​
 β
 
1
​
 )
Optimization objective
Recall that originally we wanted to search for 
β
β that maximize the likelihood function. Since 
l
o
g
log is a monotonic transformation, our maximization objective does not change and we can confindently say that now we can equally search for 
β
β that maximize our log-likelihood. Hence we can finally write our actual objective as:

a
r
g
m
a
x
β
[
∑
X
y
i
log
⁡
σ
(
x
i
β
)
+
(
1
−
y
i
)
log
⁡
(
1
−
σ
(
x
i
β
)
)
]
=
=
a
r
g
m
i
n
β
−
[
∑
X
y
i
log
⁡
σ
(
x
i
β
)
+
(
1
−
y
i
)
log
⁡
(
1
−
σ
(
x
i
β
)
)
]
argmax 
β
​
 [ 
X
∑
​
 y 
i
​
 logσ(x 
i
​
 β)+(1−y 
i
​
 )log(1−σ(x 
i
​
 β))]=
=argmin 
β
​
 −[ 
X
∑
​
 y 
i
​
 logσ(x 
i
​
 β)+(1−y 
i
​
 )log(1−σ(x 
i
​
 β))]
where 
σ
σ is the sigmoid. This function we're trying to minimize is also called Binary Cross Entropy loss function (BCE). To find the minimum we would need to take the gradient of this LLF (Log-Likelihood Function), or find a vector of derivatives with respect to every individual 
β
j
β 
j
​
 .

Step 1
To do that we're going to use a chain rule, that describes relational change in variables that our original function is made of. In our case the log-likeligood function depends on sigmoid 
σ
σ, 
σ
σ depends on 
X
β
Xβ and 
X
β
Xβ finally depends on 
β
j
β 
j
​
 , hence:

∂
L
L
F
∂
β
j
=
∂
L
L
F
∂
σ
∂
σ
∂
[
X
β
]
∂
[
X
β
]
β
j
=
−
∑
i
=
1
n
(
y
(
i
)
1
σ
(
x
(
i
)
β
)
−
(
1
−
y
(
i
)
)
1
1
−
σ
(
x
(
i
)
β
)
)
∂
σ
∂
[
x
(
i
)
β
]
∂β 
j
​
 
∂LLF
​
 = 
∂σ
∂LLF
​
  
∂[Xβ]
∂σ
​
  
β 
j
​
 
∂[Xβ]
​
 
=− 
i=1
∑
n
​
 (y 
(i)
  
σ(x 
(i)
 β)
1
​
 −(1−y 
(i)
 ) 
1−σ(x 
(i)
 β)
1
​
 ) 
∂[x 
(i)
 β]
∂σ
​
 
Step 2
Then we use a derivative of the sigmoid function, that is 
∂
σ
(
x
)
∂
x
=
σ
(
x
)
(
1
−
σ
(
x
)
)
∂x
∂σ(x)
​
 =σ(x)(1−σ(x)):

−
∑
i
=
1
n
(
y
(
i
)
1
σ
(
x
(
i
)
β
)
−
(
1
−
y
(
i
)
)
1
1
−
σ
(
x
(
i
)
β
)
)
σ
(
x
(
i
)
β
)
(
1
−
σ
(
x
(
i
)
β
)
)
(
∗
)
∂
[
x
(
i
)
β
]
∂
β
j
=
−
∑
i
=
1
n
(
y
(
i
)
(
1
−
σ
(
x
(
i
)
β
)
)
−
(
1
−
y
(
i
)
)
σ
(
x
(
i
)
β
)
)
x
j
(
i
)
=
−
∑
i
=
1
n
(
y
(
i
)
−
σ
(
x
(
i
)
β
)
)
x
j
(
i
)
=
∑
i
=
1
n
(
σ
(
x
(
i
)
β
)
−
y
(
i
)
)
x
j
(
i
)
.
− 
i=1
∑
n
​
 (y 
(i)
  
σ(x 
(i)
 β)
1
​
 −(1−y 
(i)
 ) 
1−σ(x 
(i)
 β)
1
​
 )
σ(x 
(i)
 β)(1−σ(x 
(i)
 β)) 
(∗)
  
∂β 
j
​
 
∂[x 
(i)
 β]
​
 
=− 
i=1
∑
n
​
 (y 
(i)
 (1−σ(x 
(i)
 β))−(1−y 
(i)
 )σ(x 
(i)
 β))x 
j
(i)
​
 
=− 
i=1
∑
n
​
 (y 
(i)
 −σ(x 
(i)
 β))x 
j
(i)
​
 
= 
i=1
∑
n
​
 (σ(x 
(i)
 β)−y 
(i)
 )x 
j
(i)
​
 .
The result sum can be then rewritten in a more convenient gradient matrix form as:

X
T
(
σ
(
X
β
)
−
Y
)
X 
T
 (σ(Xβ)−Y)
Then we can finally use gradient descent in order to iteratively update our parameters:

β
t
+
1
=
β
t
−
η
[
X
T
(
σ
(
X
β
t
)
−
Y
)
]
β 
t+1
​
 =β 
t
​
 −η[X 
T
 (σ(Xβ 
t
​
 )−Y)]

"""

import numpy as np

def train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:
    """
    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.
    """
    def sigmoid(x):
        return 1 / (1 + np.exp(-x))

    y = y.reshape(-1, 1)
    X = np.hstack((np.ones((X.shape[0], 1)), X))
    B = np.zeros((X.shape[1], 1))
    losses = []

    for _ in range(iterations):
        y_pred = sigmoid(X @ B)
        B -= learning_rate * X.T @ (y_pred - y)
        loss = -np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))
        losses.append(round(loss, 4))

    return B.flatten().round(4).tolist(), losses
